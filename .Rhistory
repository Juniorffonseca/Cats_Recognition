dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(lr = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model <- keras_model_sequential() %>%
# adding the first convolution layer with 16 3by3 filters
# we add an additional dimension in the input shape since convolutions operate over 3D tensors
# the input shape tells the network that the first layer should expect
# images of 150 by 150 pixels with a color depth of 3 ie RGB images
layer_conv_2d(input_shape = c(150, 150, 3), filters = 16, kernel_size = c(3, 3), activation = 'relu') %>%
# adding a max pooling layer which halves the dimensions
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
# adding a second convolution layer with 32 filters
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu') %>%
# adding a pooling layer
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
# increasing number of filters as image size decreases
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2))
model <- model %>%
layer_flatten() %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dense(units = 1, activation ='sigmoid')
model %>% summary()
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(lr = 0.001),
metrics = 'accuracy'
)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(learning_rate = 0.001),
metrics = 'accuracy'
)
history <- model %>% fit_generator(
generator = train_data_gen,
# Total number of steps (batches of samples) to yield
#before declaring one epoch finished and starting the next epoch.
steps_per_epoch = 90,
# An epoch is an iteration over the entire data provided
epochs = 15,
validation_data = validation_images,
validation_steps = 5
)
history <- model %>% fit(
generator = train_data_gen,
# Total number of steps (batches of samples) to yield
#before declaring one epoch finished and starting the next epoch.
steps_per_epoch = 90,
# An epoch is an iteration over the entire data provided
epochs = 15,
validation_data = validation_images,
validation_steps = 5
)
history <- model %>% fit(
generator = train_images,
# Total number of steps (batches of samples) to yield
#before declaring one epoch finished and starting the next epoch.
steps_per_epoch = 90,
# An epoch is an iteration over the entire data provided
epochs = 15,
validation_data = validation_images,
validation_steps = 5
)
?optimizer_adam
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit_generator(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
reticulate::py_install("pillow")
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
conda_install("r-reticulate", "scipy")
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
reticulate::py_last_error()
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 224
height <- 224
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
test_images <- flow_images_from_directory(path_train,
validation_images,
target_size = target_size,
class_mode = "categorical",
classes = label_list,
shuffle = F,
seed = 2021)
test_images <- flow_images_from_directory(path_train,
validation_images,
target_size = target_size,
class_mode = "categorical",
classes = label_list,
shuffle = F,
seed = 2021)
View(validation_images)
model %>% evaluate_generator(validation_images,
steps = validation_images$n)
model %>% evaluate(validation_images,
steps = validation_images$n)
validation_steps = len(validation_images)/batch_size
?len
?length
validation_steps = length(validation_images)/batch_size
model %>% evaluate(validation_images,
steps = validation_images$n)
?evaluate
model %>% evaluate(validation_images,
steps = validation_images$n, epochs = epochs, steps_per_epoch = validation_images %/% batch_size)
View(hist)
output_n <- length(label_list)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model %>% evaluate(validation_images,
steps = validation_images$n, epochs = epochs, steps_per_epoch = validation_images %/% batch_size)
model %>% evaluate(validation_images)
