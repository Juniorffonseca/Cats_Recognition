train_image_files_path <- file.path(path, "cats", "Training")
valid_image_files_path <- file.path(path, "cats", "Test")
train_data_gen = image_data_generator(
rescale = 1/255
)
valid_data_gen <- image_data_generator(
rescale = 1/255
)
train_image_array_gen <- flow_images_from_directory(train_image_files_path,
train_data_gen,
target_size = target_size,
class_mode = 'categorical',
classes = '',
seed = 42)
path <- "dados_brutos"
train_image_files_path <- file.path(path, "cats", "Training")
valid_image_files_path <- file.path(path, "cats", "Test")
train_data_gen = image_data_generator(
rescale = 1/255
)
valid_data_gen <- image_data_generator(
rescale = 1/255
)
train_image_array_gen <- flow_images_from_directory(train_image_files_path,
train_data_gen,
target_size = target_size,
class_mode = 'categorical',
classes = '',
seed = 42)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path <- "dados_brutos"
train_image_files_path <- file.path(path, "cats", "Training")
valid_image_files_path <- file.path(path, "cats", "Test")
train_data_gen = image_data_generator(
rescale = 1/255
)
valid_data_gen <- image_data_generator(
rescale = 1/255
)
train_image_array_gen <- flow_images_from_directory(train_image_files_path,
train_data_gen,
target_size = target_size,
class_mode = 'categorical',
classes = '',
seed = 42)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path <- "dados_brutos"
train_image_files_path <- file.path(path, "cats", "Training")
valid_image_files_path <- file.path(path, "cats", "Test")
train_data_gen = image_data_generator(
rescale = 1/255
)
valid_data_gen <- image_data_generator(
rescale = 1/255
)
train_image_array_gen <- flow_images_from_directory(train_image_files_path,
train_data_gen,
target_size = target_size,
class_mode = 'categorical',
classes = '',
seed = 42)
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
rm(valid_data_gen)
rm(path)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = label_list,
seed = 2021)
View(train_images)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'cat',
seed = 2021)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'a',
seed = 2021)
path_train <- "dados_brutos/"
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'a',
seed = 2021)
?flow_images_from_directory
train_data_gen <- image_data_generator(rescale = 1/64,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'a',
seed = 2021)
?image_data_generator
train_data_gen <- image_data_generator(rotation_range = 1/180,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'a',
seed = 2021)
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rotation_range = 1/180,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'a',
seed = 2021)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'a',
seed = 2021)
path_train <- "dados_brutos/cats"
train_data_gen <- image_data_generator(rotation_range = 1/180,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'a',
seed = 2021)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/cats"
train_data_gen <- image_data_generator(rotation_range = 1/180,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'label_list',
seed = 2021)
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'label_list',
seed = 2021)
View(train_data_gen)
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'label_list',
seed = 2021)
datagen = ImageDataGenerator()
datagen = image_data_generator
datagen = image_data_generator()
View(datagen)
train_data = datagen.flow_from_directory('./train')
train_data = datagen.flow_from_directory('./dados_brutos')
rm(datagen)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'label_list',
seed = 2021)
flow_images_from_directory()
?flow_images_from_directory
train_images <- flow_images_from_directory(path_train,
train_data_gen,
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'label_list',
seed = 2021)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
classes = 'label_list',
seed = 2021)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
table(train_images$classes)
table(validation_images$classes)
plot(as.raster(train_images[[1]][[1]][17,,,]))
plot(as.raster(train_images[[1]][[1]]))
pip install pillow
reticulate::py_install("pillow",env=tf)
plot(as.raster(train_images[[1]][[1]][17,,,]))
plot(as.raster(train_images[[1]][[1]][17,,,]))
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(width, height, 3))
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(lr = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(lr = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model <- keras_model_sequential() %>%
# adding the first convolution layer with 16 3by3 filters
# we add an additional dimension in the input shape since convolutions operate over 3D tensors
# the input shape tells the network that the first layer should expect
# images of 150 by 150 pixels with a color depth of 3 ie RGB images
layer_conv_2d(input_shape = c(150, 150, 3), filters = 16, kernel_size = c(3, 3), activation = 'relu') %>%
# adding a max pooling layer which halves the dimensions
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
# adding a second convolution layer with 32 filters
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = 'relu') %>%
# adding a pooling layer
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
# increasing number of filters as image size decreases
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2))
model <- model %>%
layer_flatten() %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dense(units = 1, activation ='sigmoid')
model %>% summary()
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(lr = 0.001),
metrics = 'accuracy'
)
model %>%
compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(learning_rate = 0.001),
metrics = 'accuracy'
)
history <- model %>% fit_generator(
generator = train_data_gen,
# Total number of steps (batches of samples) to yield
#before declaring one epoch finished and starting the next epoch.
steps_per_epoch = 90,
# An epoch is an iteration over the entire data provided
epochs = 15,
validation_data = validation_images,
validation_steps = 5
)
history <- model %>% fit(
generator = train_data_gen,
# Total number of steps (batches of samples) to yield
#before declaring one epoch finished and starting the next epoch.
steps_per_epoch = 90,
# An epoch is an iteration over the entire data provided
epochs = 15,
validation_data = validation_images,
validation_steps = 5
)
history <- model %>% fit(
generator = train_images,
# Total number of steps (batches of samples) to yield
#before declaring one epoch finished and starting the next epoch.
steps_per_epoch = 90,
# An epoch is an iteration over the entire data provided
epochs = 15,
validation_data = validation_images,
validation_steps = 5
)
?optimizer_adam
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit_generator(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
reticulate::py_install("pillow")
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
# Carregando pacotes -----------------------------------------------------------
library(tidyverse)
library(keras)
library(tensorflow)
library(reticulate)
# Training
setwd("C:/Users/anonb/Documents/Cats_Recognition")
label_list <- dir("dados_brutos/")
output_n <- length(label_list)
save(label_list, file="label_list.R")
# Tamanho das imagens
width <- 64
height <- 64
target_size <- c(width, height)
rgb <- 3 #color channels
path_train <- "dados_brutos/"
train_data_gen <- image_data_generator(rescale = 1/255,
validation_split = .2)
train_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'training',
target_size = target_size,
class_mode = "categorical",
shuffle=F,
seed = 2021)
validation_images <- flow_images_from_directory(path_train,
train_data_gen,
subset = 'validation',
target_size = target_size,
class_mode = "categorical",
seed = 2021)
mod_base <- application_xception(weights = 'imagenet',
include_top = FALSE, input_shape = c(71, 71, 3))
freeze_weights(mod_base)
model_function <- function(learning_rate = 0.001,
dropoutrate=0.2, n_dense=1024){
k_clear_session()
model <- keras_model_sequential() %>%
mod_base %>%
layer_global_average_pooling_2d() %>%
layer_dense(units = n_dense) %>%
layer_activation("relu") %>%
layer_dropout(dropoutrate) %>%
layer_dense(units=output_n, activation="softmax")
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
model
batch_size <- 32
epochs <- 6
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
conda_install("r-reticulate", "scipy")
hist <- model %>% fit(
train_images,
steps_per_epoch = train_images$n %/% batch_size,
epochs = epochs,
validation_data = validation_images,
validation_steps = validation_images$n %/% batch_size,
verbose = 2
)
